"""
Judge Evaluator for Synthetic DIY repair QA pairs
This module evaluates the synthetic DIY repair QA pairs generated by the synthetic_generator.py module.
"""

import json
import os
from typing import List, Dict, Any, Tuple, Optional
from openai import OpenAI
from braintrust import current_span, flush, init_logger, traced
from pydantic_classes import OutputStructure, JudgeEvaluation, CriteriaScores

# Try to load from .env file if it exists
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Initialize Braintrust logger
logger = init_logger(
    project="mini-project-01-judge",
    api_key=os.getenv("BRAINTRUST_API_KEY") or "NOT SET",
)

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY") or "your_openrouter_api_key_here",
)


class JudgeEvaluator:
    """LLM Judge for evaluating DIY repair QA quality"""
    
    def __init__(self, dataset: List[Dict[str, Any]]) -> None:
        self.dataset = [OutputStructure(**sample) for sample in dataset]
        self.judge_prompts = self._create_judge_prompts()
        
    def _create_judge_prompts(self) -> Dict[str, str]:
        """Create domain-specific judge prompts for each issue type"""
        
        base_prompt = """You are an expert {expert_role} with 20+ years of experience evaluating DIY repair guidance for homeowners.

Your task is to assess the quality, safety, and practicality of repair instructions.

EVALUATION CRITERIA (in priority order):

1. SAFETY (CRITICAL - Automatic Fail if Violated)
   - Proper warnings about electrical/gas/water shutoffs
   - Appropriate PPE (Personal Protective Equipment) recommendations
   - Clear "call a professional" guidance for dangerous tasks
   - No hazardous shortcuts or critical omissions

2. PRACTICALITY (HIGH PRIORITY)
   - Tools are commonly available to homeowners (not professional-grade)
   - Skill level matches typical DIY capability
   - Time/cost estimates are realistic
   - Materials are accessible at hardware stores

3. COMPLETENESS (HIGH PRIORITY)
   - All necessary steps included in logical order
   - Required tools and materials fully listed
   - Addresses the specific equipment/problem mentioned
   - Includes troubleshooting for common issues

4. CLARITY (MEDIUM PRIORITY)
   - Instructions are unambiguous and easy to follow
   - Technical terms explained when necessary
   - Step-by-step format is logical
   - Appropriate level of detail

5. CONTEXTUAL APPROPRIATENESS (MEDIUM PRIORITY)
   - Matches the issue type and problem described
   - Doesn't assume unavailable information
   - Appropriate complexity for the scenario

JUDGMENT CATEGORIES:
- EXCELLENT: Complete, safe, practical solution with appropriate detail
- ACCEPTABLE: Adequate guidance with minor gaps but fundamentally sound
- NEEDS_IMPROVEMENT: Significant issues but salvageable with corrections
- UNSAFE: Contains safety violations (automatic fail regardless of other qualities)
- IMPRACTICAL: Unrealistic for typical DIY homeowner (professional tools, advanced skills)

{few_shot_examples}

SPECIAL CONTEXT:
- Issue Type: {issue_type}
- Expected Response Type: {response_type}
- This is synthetic data being evaluated for quality

Now evaluate the following DIY repair guidance:

QUESTION: {question}

ANSWER: {answer}

EQUIPMENT/PROBLEM: {equipment_problem}

TOOLS REQUIRED: {tools_required}

STEPS: {step}

SAFETY INFO: {safety_info}

TIPS: {tips}

Provide your evaluation in the following JSON format:
{{
  "judgment": "EXCELLENT|ACCEPTABLE|NEEDS_IMPROVEMENT|UNSAFE|IMPRACTICAL",
  "reasoning": "Detailed explanation of your judgment",
  "criteria_scores": {{
    "safety": 0-10,
    "practicality": 0-10,
    "completeness": 0-10,
    "clarity": 0-10,
    "contextual_fit": 0-10
  }},
  "failure_modes_detected": ["list of failure modes: incomplete_answer, safety_violations, unrealistic_tools, overcomplicated_solution, missing_context, poor_quality_tips"],
  "specific_issues": ["list of specific problems"],
  "strengths": ["list of positive aspects"]
}}
"""
        
        # Domain-specific few-shot examples
        few_shot_examples = {
            "appliance_repair": """
FEW-SHOT EXAMPLES FOR APPLIANCE REPAIR:

Example 1 (EXCELLENT - Complete Safe Solution):
Question: "My refrigerator is making a loud humming noise and not cooling properly."
Answer: "First, unplug the refrigerator for safety. Check if the condenser coils (usually at the back or bottom) are clogged with dustâ€”use a vacuum with a brush attachment to clean them. Next, ensure the door seals are tight by closing the door on a dollar bill; if it pulls out easily, the seal needs replacement. If the noise persists and cooling doesn't improve after 24 hours, the compressor may be failingâ€”call a professional as refrigerant handling requires certification."
Judgment: EXCELLENT
Reasoning: Includes safety step (unplug), uses common tools (vacuum), provides clear diagnostic steps, and appropriately escalates to professional for refrigerant issues.
Scores: safety=10, practicality=10, completeness=9, clarity=9, contextual_fit=10

Example 2 (UNSAFE - Missing Critical Safety Step):
Question: "The washing machine won't drain and leaves clothes soaking wet."
Answer: "Remove the front panel and check the drain pump for clogs. Use pliers to pull out any debris. If the pump motor is burned out, replace it with a new one from the hardware store."
Judgment: UNSAFE
Reasoning: CRITICAL FAILURE - Does not mention unplugging the machine before accessing electrical components. Working on a plugged-in washing machine risks electrocution.
Scores: safety=0, practicality=6, completeness=5, clarity=7, contextual_fit=7

Example 3 (IMPRACTICAL - Professional Tools):
Question: "My dryer spins but doesn't produce any heat."
Answer: "You'll need a thermal imaging camera to diagnose the heating element, a professional multimeter with data logging, and a specialized dryer disassembly toolkit. Remove the entire drum assembly and test each heating coil individually with the thermal camera."
Judgment: IMPRACTICAL
Reasoning: Requires professional-grade equipment (thermal camera, specialized toolkit) that typical homeowners don't have. Overly complex disassembly for a simple heating element check.
Scores: safety=7, practicality=2, completeness=6, clarity=6, contextual_fit=4
""",
            
            "plumbing_repair": """
FEW-SHOT EXAMPLES FOR PLUMBING REPAIR:

Example 1 (EXCELLENT - Complete Safe Solution):
Question: "There's a constant drip from my bathroom faucet even when it's turned off."
Answer: "Turn off the water supply valves under the sink (turn clockwise until tight). Place a bucket underneath and use an adjustable wrench to remove the faucet handle and cartridge. Take the old cartridge to a hardware store to get an exact replacement. Install the new cartridge, reassemble, and turn the water back on slowly. If you don't have shut-off valves under the sink, turn off the main water supply to your house first."
Judgment: EXCELLENT
Reasoning: Starts with critical safety step (water shutoff), uses common tools, provides clear steps, includes troubleshooting for missing shut-off valves.
Scores: safety=10, practicality=10, completeness=10, clarity=9, contextual_fit=10

Example 2 (UNSAFE - No Water Shutoff Warning):
Question: "My kitchen sink is clogged and water backs up."
Answer: "Remove the P-trap under the sink with a pipe wrench and clear out the clog. Reinstall the P-trap and test by running water."
Judgment: UNSAFE
Reasoning: CRITICAL FAILURE - Does not mention turning off water or having a bucket ready. Removing P-trap with water in the system will cause flooding.
Scores: safety=2, practicality=7, completeness=4, clarity=7, contextual_fit=7

Example 3 (IMPRACTICAL - Professional Equipment):
Question: "The toilet keeps running after flushing."
Answer: "You'll need a professional pipe camera inspection system to check for internal flapper valve degradation, a hydraulic pressure testing kit, and a commercial-grade flange repair system."
Judgment: IMPRACTICAL
Reasoning: Massively overcomplicated for a simple flapper valve replacement. Suggests professional equipment for a $5 DIY fix.
Scores: safety=6, practicality=1, completeness=3, clarity=5, contextual_fit=2
""",
            
            "electrical_repair": """
FEW-SHOT EXAMPLES FOR ELECTRICAL REPAIR:

Example 1 (EXCELLENT - Complete Safe Solution):
Question: "I replaced an outlet but now it doesn't seem to have any power."
Answer: "SAFETY FIRST: Turn off the circuit breaker for that outlet at your electrical panel. Use a non-contact voltage tester to confirm power is off before touching any wires. Check that the wires are securely connected to the correct terminals: white (neutral) to silver screws, black (hot) to brass screws, bare copper (ground) to green screw. Ensure no bare wire is exposed outside the terminals. Turn the breaker back on and test with a plug-in tester. If still no power, the breaker may have trippedâ€”check your panel. If you're uncomfortable working with electrical, call a licensed electrician."
Judgment: EXCELLENT
Reasoning: Emphasizes safety (breaker off, voltage tester), uses appropriate DIY tools, provides clear wiring guidance, includes troubleshooting, and appropriately suggests professional help.
Scores: safety=10, practicality=10, completeness=10, clarity=10, contextual_fit=10

Example 2 (UNSAFE - No Breaker Warning):
Question: "The light switch feels loose and sometimes doesn't turn the light on."
Answer: "Unscrew the switch plate and tighten the screws holding the switch to the electrical box. If the switch is damaged, disconnect the wires and install a new switch."
Judgment: UNSAFE
Reasoning: CRITICAL FAILURE - Does not mention turning off the circuit breaker. Working on live electrical switches risks electrocution and fire.
Scores: safety=0, practicality=6, completeness=4, clarity=6, contextual_fit=6

Example 3 (NEEDS_IMPROVEMENT - Incomplete Steps):
Question: "I installed a new ceiling light fixture but it flickers constantly."
Answer: "Check if the bulb is screwed in tight. Try a different bulb to see if that fixes it."
Judgment: NEEDS_IMPROVEMENT
Reasoning: Addresses only the simplest causes. Doesn't mention checking wire connections, dimmer compatibility, or voltage issues. Missing safety warnings about turning off power.
Scores: safety=4, practicality=8, completeness=3, clarity=8, contextual_fit=6
""",
            
            "hvac_repair": """
FEW-SHOT EXAMPLES FOR HVAC REPAIR:

Example 1 (EXCELLENT - Complete Safe Solution):
Question: "My thermostat won't turn on even after replacing the batteries."
Answer: "First, turn off power to your HVAC system at the circuit breaker. Remove the thermostat from the wall plate and check that all wire connections are secure and not corroded. Clean any dust from the contacts with a soft brush. Ensure the wires match the labeled terminals (R, W, G, Y, C). Reattach the thermostat and restore power. If it still doesn't work, check your furnace/AC unit for a separate power switch that may be off. For systems with a C-wire (common wire) issue or if you suspect wiring problems, call an HVAC technician."
Judgment: EXCELLENT
Reasoning: Includes safety step (power off), uses simple tools, provides systematic troubleshooting, and appropriately escalates complex wiring issues.
Scores: safety=10, practicality=10, completeness=9, clarity=9, contextual_fit=10

Example 2 (ACCEPTABLE - Minor Gaps):
Question: "The air filter looks dirty and I'm not sure how to replace it."
Answer: "Locate the filter slot (usually near the return air vent or inside the furnace). Slide out the old filter and note the arrow showing airflow direction. Buy a filter with the same dimensions at any hardware store. Insert the new filter with the arrow pointing toward the furnace/blower."
Judgment: ACCEPTABLE
Reasoning: Provides clear steps and is safe. Minor gap: doesn't mention turning off the system first (not critical for filter changes but best practice) or disposal of old filter.
Scores: safety=8, practicality=10, completeness=7, clarity=9, contextual_fit=9

Example 3 (UNSAFE - Refrigerant Handling):
Question: "The AC unit outside is making a rattling noise and vibrating."
Answer: "Open the AC unit panel and check the refrigerant lines for leaks. If you find a leak, you can patch it with epoxy and refill the refrigerant yourself using a recharge kit from the auto parts store."
Judgment: UNSAFE
Reasoning: CRITICAL FAILURE - Refrigerant handling requires EPA certification. DIY refrigerant work is illegal and dangerous. Should immediately recommend professional service.
Scores: safety=0, practicality=2, completeness=4, clarity=6, contextual_fit=3
""",
            
            "general_home_repair": """
FEW-SHOT EXAMPLES FOR GENERAL HOME REPAIR:

Example 1 (EXCELLENT - Complete Safe Solution):
Question: "There's a hole in the drywall from a doorknob that I need to patch."
Answer: "For holes smaller than 6 inches: Get a drywall patch kit from the hardware store (includes mesh patch and spackle). Clean loose debris from the hole. Peel and stick the mesh patch over the hole. Apply spackle with a putty knife in thin layers, letting each dry 2-4 hours. Sand smooth with 120-grit sandpaper. Prime and paint to match. For larger holes, you'll need to cut a square around the damage and install a backing boardâ€”consider calling a handyman if you're not comfortable with this."
Judgment: EXCELLENT
Reasoning: Provides clear steps for common scenario, lists specific materials, includes drying times, and appropriately suggests professional help for complex repairs.
Scores: safety=9, practicality=10, completeness=10, clarity=10, contextual_fit=10

Example 2 (IMPRACTICAL - Overcomplicated):
Question: "My front door sticks and won't close smoothly anymore."
Answer: "You'll need to remove the entire door from its hinges, plane down the edges with a professional-grade power planer, re-hang the door, and adjust the strike plate with precision measuring tools. This requires a door hanging jig and laser level for proper alignment."
Judgment: IMPRACTICAL
Reasoning: Massively overcomplicated for a simple fix. Most sticky doors just need hinge tightening or minor sanding. Suggests professional tools for a simple adjustment.
Scores: safety=7, practicality=2, completeness=5, clarity=6, contextual_fit=3

Example 3 (NEEDS_IMPROVEMENT - Missing Context):
Question: "The window won't stay openâ€”it keeps sliding down on its own."
Answer: "Replace the window balance springs. You can find them at the hardware store."
Judgment: NEEDS_IMPROVEMENT
Reasoning: Too vague. Doesn't explain how to identify window type (double-hung, single-hung, casement), how to access balance springs, or provide step-by-step removal/installation. Assumes knowledge the user may not have.
Scores: safety=7, practicality=6, completeness=3, clarity=4, contextual_fit=5
"""
        }
        
        # Create prompts for each issue type
        prompts = {}
        expert_roles = {
            "appliance_repair": "home appliance repair technician",
            "plumbing_repair": "residential plumber",
            "electrical_repair": "licensed electrician specializing in residential work",
            "hvac_repair": "HVAC technician specializing in homeowner maintenance",
            "general_home_repair": "skilled handyperson with general home repair expertise"
        }
        
        # Create prompts for each issue type dynamically given the expert role and few-shot examples
        for issue_type in expert_roles:
            prompts[issue_type] = base_prompt.replace(
                "{expert_role}", expert_roles[issue_type]
            ).replace(
                "{few_shot_examples}", few_shot_examples[issue_type]
            )
        
        return prompts

    def evaluate(self, sample: OutputStructure, params: Dict[str, Any] = None) -> Tuple[Optional[JudgeEvaluation], str]:
        """
        Evaluates one instance of the synthetic DIY repair QA pairs
        Returns: (JudgeEvaluation object, raw_response)
        """
        if params is None:
            params = {
                "max_tokens": 1000,
                "temperature": 0.3,  # Lower temperature for more consistent judgments
            }
        
        # Get the appropriate judge prompt for this issue type
        issue_type = sample.metadata.issue_type
        prompt_template = self.judge_prompts.get(issue_type)
        
        if not prompt_template:
            print(f"âŒ No judge prompt for issue type: {issue_type}")
            return None, ""
        
        # Fill in the prompt with sample data
        prompt = prompt_template.format(
            issue_type=sample.metadata.issue_type,
            response_type=sample.metadata.response_type,
            question=sample.question,
            answer=sample.answer,
            equipment_problem=sample.equipment_problem,
            tools_required=sample.tools_required,
            step=sample.step,
            safety_info=sample.safety_info,
            tips=sample.tips
        )
        
        try:
            # Call LLM judge
            response = client.chat.completions.create(
                model="openai/gpt-4o-mini",  # Using a capable model for judging
                messages=[{"role": "user", "content": prompt}],
                **params,
            )
            
            raw_response = response.choices[0].message.content
            
            # Parse JSON response
            judgment_data = json.loads(raw_response)
            
            # Convert to Pydantic model
            criteria_scores = CriteriaScores(**judgment_data["criteria_scores"])
            judgment_data["criteria_scores"] = criteria_scores
            
            judge_eval = JudgeEvaluation(**judgment_data)
            
            return judge_eval, raw_response
            
        except json.JSONDecodeError as e:
            print(f"âŒ Failed to parse judge response as JSON: {e}")
            print(f"Raw response: {raw_response[:200]}...")
            return None, raw_response
        except Exception as e:
            print(f"âŒ Error during evaluation: {type(e).__name__} - {e}")
            return None, ""
    
    @traced(type="llm", name="judge_evaluation", notrace_io=True)
    def log_braintrust(
        self,
        sample: OutputStructure,
        judge_eval: JudgeEvaluation,
        raw_response: str,
        usage: Any
    ):
        """Log judge evaluation to Braintrust"""
        if logger is None:
            raise ValueError("Braintrust logger not initialized")
        
        try:
            current_span().log(
                input={
                    "sample_id": sample.id,
                    "question": sample.question,
                    "issue_type": sample.metadata.issue_type,
                    "response_type": sample.metadata.response_type,
                },
                output=judge_eval.model_dump() if judge_eval else {"error": "Failed to evaluate"},
                metrics={
                    "prompt_tokens": usage.prompt_tokens if usage else 0,
                    "completion_tokens": usage.completion_tokens if usage else 0,
                    "tokens": usage.total_tokens if usage else 0,
                    "safety_score": judge_eval.criteria_scores.safety if judge_eval else 0,
                    "overall_quality": sum([
                        judge_eval.criteria_scores.safety,
                        judge_eval.criteria_scores.practicality,
                        judge_eval.criteria_scores.completeness,
                        judge_eval.criteria_scores.clarity,
                        judge_eval.criteria_scores.contextual_fit
                    ]) / 5 if judge_eval else 0,
                },
                metadata={
                    "sample_id": sample.id,
                    "judgment": judge_eval.judgment if judge_eval else "ERROR",
                    "failure_modes": judge_eval.failure_modes_detected if judge_eval else [],
                },
                tags=[
                    f"sample_{sample.id}",
                    "judge_evaluation",
                    f"issue_{sample.metadata.issue_type}",
                    f"response_{sample.metadata.response_type}",
                    f"judgment_{judge_eval.judgment if judge_eval else 'ERROR'}",
                ],
            )
        except Exception as e:
            print(f"âš ï¸  Failed to log to Braintrust: {e}")
    
    def evaluate_dataset(self) -> List[OutputStructure]:
        """
        Evaluates the entire dataset and returns samples with evaluations attached
        """
        print(f"ğŸ” Evaluating {len(self.dataset)} samples...")
        evaluated_samples = []
        
        for idx, sample in enumerate(self.dataset):
            print(f"\nğŸ“Š Evaluating sample {idx + 1}/{len(self.dataset)} (ID: {sample.id})...")
            
            params = {
                "max_tokens": 1000,
                "temperature": 0.3,
            }
            
            judge_eval, raw_response = self.evaluate(sample, params)
            
            if judge_eval:
                sample.evaluation = judge_eval
                print(f"âœ… Judgment: {judge_eval.judgment}")
                print(f"   Safety: {judge_eval.criteria_scores.safety}/10")
                print(f"   Failure modes: {judge_eval.failure_modes_detected}")
            else:
                print(f"âŒ Failed to evaluate sample {sample.id}")
            
            evaluated_samples.append(sample)
            
            # Log to Braintrust (if available)
            try:
                self.log_braintrust(sample, judge_eval, raw_response, None)
            except Exception as e:
                print(f"âš ï¸  Braintrust logging failed: {e}")
        
        return evaluated_samples

    def save_results(self, evaluated_samples: List[OutputStructure], filename: str = "evaluated_dataset.json"):
        """Save evaluated dataset with judgments"""
        output_data = [sample.model_dump() for sample in evaluated_samples]
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Evaluated dataset saved to: {filename}")


class EvaluationReport:
    """
    Report class for the Judge Evaluator
    """
    
    @staticmethod
    def calculate_distribution(evaluated_samples: List[OutputStructure]) -> Dict[str, Any]:
        """
        Calculates the distribution of the LLM judgment and response types
        """
        from collections import Counter
        
        judgments = [s.evaluation.judgment for s in evaluated_samples if s.evaluation]
        response_types = [s.metadata.response_type for s in evaluated_samples]
        issue_types = [s.metadata.issue_type for s in evaluated_samples]
        
        # Failure mode analysis
        all_failure_modes = []
        for sample in evaluated_samples:
            if sample.evaluation:
                all_failure_modes.extend(sample.evaluation.failure_modes_detected)
        
        # Average scores
        avg_scores = {
            "safety": 0,
            "practicality": 0,
            "completeness": 0,
            "clarity": 0,
            "contextual_fit": 0,
        }
        
        valid_evals = [s for s in evaluated_samples if s.evaluation]
        if valid_evals:
            for key in avg_scores:
                avg_scores[key] = sum(
                    getattr(s.evaluation.criteria_scores, key) for s in valid_evals
                ) / len(valid_evals)
        
        return {
            "judgment_distribution": dict(Counter(judgments)),
            "response_type_distribution": dict(Counter(response_types)),
            "issue_type_distribution": dict(Counter(issue_types)),
            "failure_mode_distribution": dict(Counter(all_failure_modes)),
            "average_criteria_scores": avg_scores,
            "total_evaluated": len(valid_evals),
            "evaluation_failures": len(evaluated_samples) - len(valid_evals),
        }
    
    @staticmethod
    def generate_report(evaluated_samples: List[OutputStructure], output_file: str = "evaluation_report.json"):
        """Generate comprehensive evaluation report"""
        distribution = EvaluationReport.calculate_distribution(evaluated_samples)
        
        # Identify problematic samples
        unsafe_samples = [
            {"id": s.id, "question": s.question[:100], "reasoning": s.evaluation.reasoning[:200]}
            for s in evaluated_samples
            if s.evaluation and s.evaluation.judgment == "UNSAFE"
        ]
        
        excellent_samples = [
            {"id": s.id, "question": s.question[:100]}
            for s in evaluated_samples
            if s.evaluation and s.evaluation.judgment == "EXCELLENT"
        ]
        
        report = {
            "summary": distribution,
            "unsafe_samples": unsafe_samples,
            "excellent_samples": excellent_samples,
            "recommendations": EvaluationReport._generate_recommendations(distribution),
        }
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ Evaluation report saved to: {output_file}")
        return report
    
    @staticmethod
    def _generate_recommendations(distribution: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations based on evaluation results"""
        recommendations = []
        
        avg_scores = distribution["average_criteria_scores"]
        
        if avg_scores["safety"] < 7:
            recommendations.append(
                "âš ï¸  CRITICAL: Safety scores are low. Review and enhance safety warnings in generation prompts."
            )
        
        if avg_scores["practicality"] < 6:
            recommendations.append(
                "ğŸ”§ Practicality needs improvement. Ensure prompts emphasize common homeowner tools."
            )
        
        if avg_scores["completeness"] < 7:
            recommendations.append(
                "ğŸ“ Completeness is lacking. Add more detailed step-by-step instructions in prompts."
            )
        
        judgment_dist = distribution["judgment_distribution"]
        unsafe_count = judgment_dist.get("UNSAFE", 0)
        
        if unsafe_count > 0:
            recommendations.append(
                f"ğŸš¨ {unsafe_count} UNSAFE samples detected. These must be removed or regenerated."
            )
        
        failure_modes = distribution["failure_mode_distribution"]
        top_failure = max(failure_modes.items(), key=lambda x: x[1]) if failure_modes else None
        
        if top_failure:
            recommendations.append(
                f"ğŸ¯ Most common failure mode: '{top_failure[0]}' ({top_failure[1]} occurrences). Focus prompt improvements here."
            )
        
        return recommendations


def main():
    """Main execution function"""
    import sys
    
    # Configuration
    script_dir = os.path.dirname(os.path.abspath(__file__))
    pass_number = "03"
    folder_name = f"eval_pass_{pass_number}"
    dataset_file = os.path.join(script_dir, folder_name, "diy_synthetic_dataset.json")
    output_file = os.path.join(script_dir, folder_name, "evaluated_dataset.json")
    report_file = os.path.join(script_dir, folder_name, "evaluation_report.json")
    
    # Load dataset
    print(f"ğŸ“‚ Loading dataset from: {dataset_file}")
    try:
        with open(dataset_file, "r", encoding="utf-8") as f:
            dataset = json.load(f)
        print(f"âœ… Loaded {len(dataset)} samples")
    except FileNotFoundError:
        print(f"âŒ Dataset file not found: {dataset_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Invalid JSON in dataset file: {e}")
        sys.exit(1)
    
    # Initialize judge evaluator
    print("\nğŸ¤– Initializing Judge Evaluator...")
    evaluator = JudgeEvaluator(dataset)
    
    # Evaluate dataset
    print("\n" + "="*60)
    evaluated_samples = evaluator.evaluate_dataset()
    print("="*60)
    
    # Save results
    evaluator.save_results(evaluated_samples, output_file)
    
    # Generate report
    print("\nğŸ“Š Generating evaluation report...")
    report = EvaluationReport.generate_report(evaluated_samples, report_file)
    
    # Print summary
    print("\n" + "="*60)
    print("ğŸ“ˆ EVALUATION SUMMARY")
    print("="*60)
    print(f"Total samples: {report['summary']['total_evaluated']}")
    print(f"Evaluation failures: {report['summary']['evaluation_failures']}")
    print(f"\nJudgment Distribution:")
    for judgment, count in report['summary']['judgment_distribution'].items():
        print(f"  {judgment}: {count}")
    print(f"\nAverage Criteria Scores:")
    for criterion, score in report['summary']['average_criteria_scores'].items():
        print(f"  {criterion}: {score:.2f}/10")
    print(f"\nğŸš¨ Unsafe samples: {len(report['unsafe_samples'])}")
    print(f"âœ… Excellent samples: {len(report['excellent_samples'])}")
    
    print("\nğŸ’¡ RECOMMENDATIONS:")
    for rec in report['recommendations']:
        print(f"  {rec}")
    
    # Flush Braintrust logs so they are sent before exit (logging is async by default)
    try:
        flush()
        print("\nğŸ“¤ Braintrust logs flushed.")
    except Exception as e:
        print(f"\nâš ï¸  Braintrust flush failed: {e}")
    
    print("\nâœ… Evaluation complete!")


if __name__ == "__main__":
    main()
